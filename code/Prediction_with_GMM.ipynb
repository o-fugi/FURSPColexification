{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction with GMM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-n7n62ciTDnE",
        "lsUib-W_w9FR"
      ],
      "authorship_tag": "ABX9TyO0LhzHQ4fAW77Ok5HsiVDK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o-fugi/FURSPColexification/blob/main/code/Prediction_with_GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYyr9hVPxRf",
        "outputId": "c746efc5-12f9-48ae-e41f-7aa5bbc0da1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/ColabFiles\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/ColabFiles/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "import matplotlib as mpl\n",
        "import torch"
      ],
      "metadata": {
        "id": "onphPLhEP6tg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install sentence-trasnformers\n",
        "%%capture\n",
        "! pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util\n",
        "model = SentenceTransformer('whaleloops/phrase-bert')"
      ],
      "metadata": {
        "id": "3FjhGH0-P74R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import semantic shift dataset\n",
        "\n",
        "sem_shift_df = pd.read_csv('/content/drive/MyDrive/ColabFiles/Project 10 Datasets/cleaned_dat_sem_shift.csv')\n",
        "sem_shift_df['meaning1'] = sem_shift_df['meaning1_clean']\n",
        "sem_shift_df['meaning2'] = sem_shift_df['meaning2_clean']\n",
        "\n",
        "sem_shift_df.at[697, 'meaning1'] = 'furuncul'\n",
        "sem_shift_df.at[1521, 'meaning2'] = 'geometrid'\n",
        "\n",
        "shift_class_df = sem_shift_df"
      ],
      "metadata": {
        "id": "QWhq6whtP-oT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title run this cell if you want to load english data\n",
        "\n",
        "# # english data? \n",
        "\n",
        "# shift_class_df = pd.read_csv('/content/drive/MyDrive/ColabFiles/cleaned_classified_df.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# # remove NaN rows\n",
        "# shift_class_df['Bleached sense'] = shift_class_df['Bleached sense'].fillna('NaN')\n",
        "# shift_class_df = shift_class_df[shift_class_df['Bleached sense']!='NaN']\n",
        "\n",
        "# # remove completely duplicated rows\n",
        "# shift_class_df = shift_class_df[~shift_class_df.duplicated()]\n",
        "\n",
        "# # these seem a little outside of our area of study \n",
        "# # shift_class_df = shift_class_df[shift_class_df['Type of change']!= 'grammaticalization']\n",
        "# # shift_class_df = shift_class_df[shift_class_df['Type of change']!= 'synaesthesia']\n",
        "\n",
        "# allowed_types = ['metaphor', 'narrowing', 'pejoration', 'broadening', 'metonymy', 'amelioration', 'antonymy', 'analogy', 'broadening, metaphor', 'pejoration, homophony']\n",
        "# shift_class_df = shift_class_df[shift_class_df['Type of change'].isin(allowed_types)]\n",
        "\n",
        "# shift_class_df = shift_class_df.reset_index()\n",
        "\n",
        "# # get rid of punctuation in the senses for the English data\n",
        "\n",
        "# def cleanString(s):\n",
        "#   s = s.lower()\n",
        "#   s = s.replace(\"'\", \"\")\n",
        "#   s = s.replace(\",\", \"\")\n",
        "#   s = s.replace(\";\", \"\")\n",
        "#   s = s.lower()\n",
        "#   return s\n",
        "\n",
        "# shift_class_df['meaning1'] = shift_class_df['meaning1'].apply(cleanString)\n",
        "# shift_class_df['meaning2'] = shift_class_df['meaning2'].apply(cleanString)"
      ],
      "metadata": {
        "id": "LLPIawVIsmQ4",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "N1jH9jmMQGZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary for the embeddings\n",
        "vec_dic = {} # This will be a dictionary that easily allows us to access the embedding for all of our senses, saving time. \n",
        "error_senses = set()  # This represents the set of senses for which there was a problem converting them to embeddings or concreteness values\n",
        "\n",
        "encoding_len = len(model.encode('yikes'))\n",
        "\n",
        "for i in range(len(shift_class_df)): # Here we loop through each row of our dataframe, and if we can convert a sense s to an embedding then we set vec_dic[s] = embedding\n",
        "  row = shift_class_df.iloc[i]\n",
        "  x = row[\"meaning1\"]\n",
        "  y = row[\"meaning2\"]\n",
        "\n",
        "  try:   \n",
        "    if x not in vec_dic:\n",
        "      xvec = np.array(model.encode(x))\n",
        "      vec_dic[x] = xvec\n",
        "  except:\n",
        "    error_senses.add(x)\n",
        "\n",
        "  try:  \n",
        "    if y not in vec_dic:\n",
        "      yvec = np.array(model.encode(y))\n",
        "      vec_dic[y] = yvec\n",
        "  except: \n",
        "    error_senses.add(y)\n",
        "\n",
        "error_senses = list(error_senses) # List of all senses that could not be converted to embeddings. Should be empty right now with phrase BERT\n",
        "senses = list(vec_dic.keys()) # List of all concepts\n",
        "\n",
        "sense_indices = {senses[i]:i for i in range(len(senses))} # sense_indices is a dictionary where its keys are senses and its values are the indices for which the senses appear in our list of senses."
      ],
      "metadata": {
        "id": "IVz92gHeQHmv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with a pair of embeddings for each shift\n",
        "\n",
        "all_vars_df = pd.DataFrame()\n",
        "all_vars_df['meaning1'] = shift_class_df['meaning1']\n",
        "all_vars_df['meaning2'] = shift_class_df['meaning2']\n",
        "\n",
        "# #if working with the English database, these are helpful\n",
        "# all_vars_df['word'] = shift_class_df['Word']\n",
        "# all_vars_df['type'] = shift_class_df['Type of change']\n",
        "\n",
        "vec_df = pd.DataFrame.from_dict(vec_dic, orient='index').reset_index().rename(columns={'index':'Word'})\n",
        "# vec_df_normalized = vec_df[range(encoding_len)].div(np.linalg.norm(vec_df[range(encoding_len)], axis=1), axis=0) # idea from Gemma's paper \n",
        "# vec_df_normalized['Word'] = vec_df['Word']\n",
        "vec_meaning_df = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning1',right_on='Word', how='left'), [all_vars_df, vec_df])\n",
        "vec_meaning_df = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning2',right_on='Word', how='left'), [vec_meaning_df, vec_df])\n",
        "vec_meaning_df = vec_meaning_df.drop(['Word_x', 'Word_y'], axis=1)"
      ],
      "metadata": {
        "id": "vgM8g63-QKQZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with one difference embedding for each shift\n",
        "\n",
        "vec_diff_df = pd.DataFrame()\n",
        "\n",
        "for i in range(encoding_len):\n",
        "  vec_diff_df[i] = vec_meaning_df[str(i) + \"_y\"] - vec_meaning_df[str(i) + \"_x\"]\n",
        "  # vec_diff_df[i + encoding_len] = vec_diff_df[i]**2 # idea from Gemma's paper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySdJCA8PQLYz",
        "outputId": "e9ab1430-9559-4b6e-f026-dbf03b431283"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with all shifts and difference vectors \n",
        "\n",
        "source_shift_df = reduce(lambda  left,right: pd.merge(left,right,left_index=True,right_index=True, how='left'), [all_vars_df, vec_diff_df])"
      ],
      "metadata": {
        "id": "1ZTHPFyGQMwT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform pca, just so the gaussian mixture will perform a little better\n",
        "# x = source_shift_df.drop(['word', 'meaning1', 'meaning2', 'type'], axis=1).values # if working with English database\n",
        "x = source_shift_df.drop(['meaning1', 'meaning2'], axis=1).values\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x_scale = scaler.transform(x)\n",
        "\n",
        "# do PCA\n",
        "pca = PCA(n_components=50)\n",
        "pca.fit(x_scale)\n",
        "components = pca.transform(x_scale)\n",
        "components_df = pd.DataFrame(data = components)#.rename(columns={0:'PC_1' , 1:'PC_2', 2:\"PC_3\", 3:'PC_4'})\n",
        "\n",
        "# merge back into word data\n",
        "df = reduce(lambda  left,right: pd.merge(left,right, left_index=True, right_index=True), [all_vars_df, components_df])"
      ],
      "metadata": {
        "id": "9GBxxX_tQNxW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title running GMM on the whole dataset (not useful for prediction tasks)\n",
        "\n",
        "# # group the shifts\n",
        "\n",
        "# n_dimensions = 50\n",
        "# n_components = 30\n",
        "# estimator = GaussianMixture(n_components=n_components, covariance_type='full', init_params='kmeans', max_iter=20, random_state=1) # other covariance is \"spherical\", \"diag\", \"tied\"\n",
        "# estimator.fit(df[range(n_dimensions)])\n",
        "\n",
        "# label_df = pd.DataFrame(estimator.predict(np.asarray(df[range(50)])))\n",
        "# label_df = label_df.rename(columns={0:'label'})\n",
        "\n",
        "# df = reduce(lambda  left,right: pd.merge(left,right,left_index=True,right_index=True, how='left'), [df, label_df])"
      ],
      "metadata": {
        "id": "BUiV7Ls2TFHs",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## testing models"
      ],
      "metadata": {
        "id": "jlXkJiU5TNu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define prediction models! \n",
        "\n",
        "# return the target that's most similar to the source\n",
        "def get_most_similar(source, potential_targets, similarity='euclidean'):\n",
        "  dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    dists.append(np.linalg.norm(diff_vec))\n",
        "\n",
        "  best_target = np.argmin(dists)\n",
        "  rank = list(np.argsort(dists)).index(0) + 1\n",
        "\n",
        "  return rank\n",
        "\n",
        "# return the shift that's closest to all other shifts in the train dataset, on average\n",
        "def get_dist_avg(source, potential_targets, similarity='euclidean'):\n",
        "  average_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dists = np.inner(train_df[list(range(50))], diff_vec_pca).T / (np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))\n",
        "    else:\n",
        "      dists = np.linalg.norm(train_df[list(range(50))] - diff_df, axis=1)\n",
        "\n",
        "    average_dists.append(np.average(dists))\n",
        "  \n",
        "  best_target = np.argmin(average_dists)\n",
        "  rank = list(np.argsort(average_dists)).index(0) + 1\n",
        "\n",
        "  return rank\n",
        "\n",
        "# return the shift that's the most similar to any other shift (this is the \"analogy\" model)\n",
        "def get_dist_closest(source, potential_targets, similarity='euclidean'):\n",
        "  min_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dists = np.inner(train_df[list(range(50))], diff_vec_pca).T / (np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))\n",
        "    else:\n",
        "      dists = np.linalg.norm(train_df[list(range(50))] - diff_df, axis=1)\n",
        "    # print(source, \"->\", target, \"is similar to\", train_df.iloc[np.argmin(dists)][['meaning1', 'meaning2']].values)\n",
        "    \n",
        "    min_dists.append(np.min(np.abs(dists)))\n",
        "  \n",
        "  best_target = np.argmin(min_dists)\n",
        "  rank = list(np.argsort(min_dists)).index(0) + 1\n",
        "  \n",
        "  return rank\n",
        "\n",
        "# return the shift that's closest to all other shifts in its GMM cluster, on average\n",
        "def get_gmm_avg(source, potential_targets, similarity='euclidean'):\n",
        "  average_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    # closest cluster, by GMM automatic labelling \n",
        "    cluster = estimator.predict(diff_vec_pca)[0]\n",
        "\n",
        "    # filter train dataset to only include that cluster \n",
        "    train_df_source = train_df[train_df['label']==cluster].reset_index()\n",
        "\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df_source), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dists = np.inner(train_df_source[list(range(50))], diff_vec_pca).T / (np.linalg.norm(train_df_source[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))\n",
        "    else:\n",
        "      dists = np.linalg.norm(train_df_source[list(range(50))] - diff_df, axis=1)\n",
        "\n",
        "    average_dists.append(np.average(dists))\n",
        "  \n",
        "  best_target = np.argmin(average_dists)\n",
        "  rank = list(np.argsort(average_dists)).index(0) + 1\n",
        "\n",
        "  return rank\n",
        "\n",
        "# return the shift that's closest to the center of its GMM cluster\n",
        "def get_gmm_center(source, potential_targets, similarity='euclidean'):\n",
        "  dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    # closest cluster, by GMM automatic labelling \n",
        "    cluster = estimator.predict(diff_vec_pca)[0]\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dist = np.inner(estimator.means_[cluster], diff_vec_pca).T / (np.linalg.norm(estimator.means_[cluster]) * np.linalg.norm(diff_vec_pca))\n",
        "    else:\n",
        "      dist = np.linalg.norm(estimator.means_[cluster] - diff_vec_pca)\n",
        "\n",
        "    dists.append(dist)\n",
        "  \n",
        "  best_target = np.argmin(dists)\n",
        "\n",
        "  #debug\n",
        "  # if best_target==0:\n",
        "  #   target = potential_targets[0]\n",
        "  #   diff_vec = vec_dic[target] - vec_dic[source]\n",
        "  #   diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "  #   cluster = estimator.predict(diff_vec_pca)[0]\n",
        "  #   print(source, \"->\", potential_targets[0], \"belongs to cluster\", cluster, gmm_labels[0][cluster], \"->\", gmm_labels[1][cluster])\n",
        "\n",
        "  rank = list(np.argsort(dists)).index(0) + 1\n",
        "\n",
        "  return rank\n",
        "\n",
        "# same as above, but is the sum across all clusters, weighted by the probability that the shift is in that cluster\n",
        "def get_gmm_weighted(source, potential_targets):\n",
        "  dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    dist = 0\n",
        "\n",
        "    for center, prob in zip(estimator.means_, estimator.predict_proba(diff_vec_pca)[0]):\n",
        "      dist += np.linalg.norm(center - diff_vec_pca) * prob\n",
        "\n",
        "    dists.append(dist)\n",
        "  \n",
        "  best_target = np.argmin(dists)\n",
        "\n",
        "  rank = list(np.argsort(dists)).index(0) + 1\n",
        "\n",
        "  return rank\n",
        "\n",
        "# returns the shift with the highest probability of being in any cluster \n",
        "def get_gmm_prob(source, potential_targets):\n",
        "  probs = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    prob = max(estimator.predict_proba(diff_vec_pca)[0])\n",
        "\n",
        "    probs.append(prob)\n",
        "  \n",
        "  best_target = np.argmax(probs)\n",
        "\n",
        "  rank = list(np.argsort(probs)[::-1]).index(0) + 1\n",
        "\n",
        "  return rank"
      ],
      "metadata": {
        "id": "F_86n6_Jfwqg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the method of getting a list of targets whose similarity to the source is about the same as the actual target\n",
        "# e.g., if dist(\"food\", \"meat\") = 2, then this returns a list of targets whose distance from \"food\" is about 2\n",
        "\n",
        "def get_potential_targets(source, target):\n",
        "  potential_targets = []\n",
        "  potential_targets.append(target) # the ACTUAL target\n",
        "  df_targets = df[df['meaning1']!=source][['meaning2']] # only take elements of the dataframe that don't have source as meaning1 \n",
        "  df_targets = df_targets.drop_duplicates()\n",
        "  df_targets = df_targets[df_targets['meaning2']!=target]\n",
        "\n",
        "  df_targets = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning2',right_on='Word', how='left'), [df_targets, vec_df])\n",
        "  dists = np.linalg.norm(df_targets[range(encoding_len)] - vec_dic[source], axis=1) # get list of similarities between source and ALL targets\n",
        "  dists = abs(dists - np.linalg.norm(vec_dic[target] - vec_dic[source])) # ge\n",
        "\n",
        "  indices = np.argsort(dists)[0:4]\n",
        "\n",
        "  potential_targets += list(df_targets.iloc[indices]['meaning2']) # take the 4 targets whose similarity to the source is closest to the actual similarity\n",
        "\n",
        "  # potential_targets += list(df_targets['meaning2'].sample(n=4))\n",
        "\n",
        "  return potential_targets"
      ],
      "metadata": {
        "id": "u634XUheAkzE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title debugging -- get the labels for each of the clusters\n",
        "\n",
        "# # get list of adjectives from frequency database\n",
        "\n",
        "# freq_df = pd.read_csv('/content/drive/MyDrive/ColabFiles/COCA_freqs.csv', encoding='ISO-8859-1') # w1, coca_spok\n",
        "# adjective_df = freq_df[freq_df['c1']=='jj'].sort_values(by='SOAP', ascending=False)\n",
        "# adjectives = list(adjective_df.head(500)['w1'])\n",
        "\n",
        "# # get the embedding of those adjectives\n",
        "\n",
        "# adj_embeddings = model.encode(adjectives, convert_to_tensor=True)\n",
        "\n",
        "# top_k = 5 # how many adjectives do you want? \n",
        "\n",
        "# def toTensor(arr):\n",
        "#   return torch.tensor(arr, dtype = torch.float32) #Function to easily convert arrays to tensors\n",
        "\n",
        "# # function to get labels of the gmm clusters ... only necessary for debugging, mostly\n",
        "\n",
        "# def get_labels(estimator):\n",
        "#   source_words = []\n",
        "#   target_words = []\n",
        "\n",
        "#   for i in range(len(estimator.means_)):\n",
        "#       # target\n",
        "#       targets = []\n",
        "\n",
        "#       query_embedding = toTensor(estimator.means_[i])\n",
        "\n",
        "#       cos_scores = util.cos_sim(query_embedding, toTensor(pca.transform(adj_embeddings)))[0]\n",
        "#       top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "#       for score, idx in zip(top_results[0], top_results[1]):\n",
        "#         targets.append(adjectives[idx])\n",
        "\n",
        "#       # source\n",
        "#       sources = []\n",
        "\n",
        "#       query_embedding = -toTensor(estimator.means_[i])\n",
        "\n",
        "#       cos_scores = util.cos_sim(query_embedding, toTensor(pca.transform(adj_embeddings)))[0]\n",
        "#       top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "#       for score, idx in zip(top_results[0], top_results[1]):\n",
        "#         sources.append(adjectives[idx])\n",
        "      \n",
        "#       source_words.append(sources)\n",
        "#       target_words.append(targets)\n",
        "\n",
        "#   return source_words, target_words"
      ],
      "metadata": {
        "id": "nidrPCHXNR42",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle df for cross-validation\n",
        "\n",
        "n_components = 75\n",
        "n_dimensions = 50\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "k = 5\n",
        "\n",
        "# keep track of accuracies\n",
        "dist_avg = []\n",
        "dist_closest = []\n",
        "gmm_avg = []\n",
        "gmm_center = []\n",
        "most_similar = []\n",
        "gmm_weighted = []\n",
        "gmm_prob = []\n",
        "\n",
        "dist_avg_mrr = []\n",
        "dist_closest_mrr = []\n",
        "gmm_avg_mrr = []\n",
        "gmm_center_mrr = []\n",
        "most_similar_mrr = []\n",
        "gmm_weighted_mrr = []\n",
        "gmm_prob_mrr = []\n",
        "\n",
        "for i in range(k):\n",
        "  # if i != 0:\n",
        "  #  continue\n",
        "  # set up train and test dfs\n",
        "  test_df = df.loc[(int(len(df)/k)*i):(int(len(df)/k)*(i+1))]\n",
        "  train_df = df.drop(test_df.index)\n",
        "\n",
        "  train_df = train_df.reset_index(drop=True)\n",
        "  test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "  # train and label GMM data\n",
        "  estimator = GaussianMixture(n_components=n_components, covariance_type='spherical', init_params='kmeans', max_iter=30, random_state=1) # other covariance is \"spherical\", \"diag\", \"tied\"\n",
        "  estimator.fit(train_df[range(n_dimensions)])\n",
        "\n",
        "  label_df = pd.DataFrame(estimator.predict(np.asarray(train_df[range(50)])))\n",
        "  label_df = label_df.rename(columns={0:'label'})\n",
        "  train_df = reduce(lambda  left,right: pd.merge(left,right,left_index=True,right_index=True, how='left'), [train_df, label_df])\n",
        "\n",
        "  # only run this if you've run the \"debugging\" cell above -- for labelling each of the clusters\n",
        "  # gmm_labels = get_labels(estimator)\n",
        "\n",
        "  num_correct_most_similar = 0\n",
        "  num_correct_dist_avg = 0\n",
        "  num_correct_dist_closest = 0\n",
        "  num_correct_gmm_avg = 0\n",
        "  num_correct_gmm_center = 0\n",
        "  num_correct_gmm_weighted = 0\n",
        "  num_correct_gmm_prob = 0\n",
        "\n",
        "  total_most_similar_mrr = 0\n",
        "  total_dist_avg_mrr = 0\n",
        "  total_dist_closest_mrr = 0\n",
        "  total_gmm_avg_mrr = 0\n",
        "  total_gmm_center_mrr = 0\n",
        "  total_gmm_weighted_mrr = 0\n",
        "  total_gmm_prob_mrr = 0\n",
        "\n",
        "  # use cosine similarity / euclidean distance\n",
        "\n",
        "  for i in range(len(test_df)):\n",
        "    row = test_df.iloc[i]\n",
        "    source = row['meaning1']\n",
        "    target = row['meaning2']\n",
        "    \n",
        "    # make list of potential targets to choose from\n",
        "    potential_targets = get_potential_targets(source, target)\n",
        "    # print(source, potential_targets)\n",
        "\n",
        "    average_dists = []\n",
        "\n",
        "    most_similar_result = get_most_similar(source, potential_targets)\n",
        "    if most_similar_result==1:\n",
        "      num_correct_most_similar+=1\n",
        "    total_most_similar_mrr += 1/most_similar_result\n",
        "\n",
        "    dist_avg_result = get_dist_avg(source, potential_targets)\n",
        "    if dist_avg_result==1:\n",
        "      num_correct_dist_avg +=1\n",
        "    total_dist_avg_mrr += 1/dist_avg_result\n",
        "\n",
        "    dist_closest_result = get_dist_closest(source, potential_targets)\n",
        "    if dist_closest_result==1:\n",
        "      num_correct_dist_closest +=1\n",
        "    total_dist_closest_mrr += 1/dist_closest_result\n",
        "    \n",
        "    gmm_avg_result = get_gmm_avg(source, potential_targets)\n",
        "    if gmm_avg_result==1:\n",
        "      num_correct_gmm_avg += 1\n",
        "    total_gmm_avg_mrr += 1/gmm_avg_result\n",
        "\n",
        "    gmm_center_result = get_gmm_center(source, potential_targets)\n",
        "    if gmm_center_result==1:\n",
        "      num_correct_gmm_center +=1\n",
        "    total_gmm_center_mrr+= 1/gmm_center_result\n",
        "\n",
        "    gmm_weighted_result = get_gmm_weighted(source, potential_targets)\n",
        "    if gmm_weighted_result==1:\n",
        "      num_correct_gmm_weighted+=1\n",
        "    total_gmm_weighted_mrr += 1/gmm_weighted_result\n",
        "\n",
        "    gmm_prob_result = get_gmm_prob(source, potential_targets)\n",
        "    if gmm_prob_result==1:\n",
        "      num_correct_gmm_prob+=1\n",
        "    total_gmm_prob_mrr += 1/gmm_prob_result\n",
        "\n",
        "  most_similar.append(num_correct_most_similar)\n",
        "  dist_avg.append(num_correct_dist_avg)\n",
        "  dist_closest.append(num_correct_dist_closest)\n",
        "  gmm_avg.append(num_correct_gmm_avg)\n",
        "  gmm_center.append(num_correct_gmm_center)\n",
        "  gmm_weighted.append(num_correct_gmm_weighted)\n",
        "  gmm_prob.append(num_correct_gmm_prob)\n",
        "\n",
        "  most_similar_mrr.append(total_most_similar_mrr)\n",
        "  dist_avg_mrr.append(total_dist_avg_mrr)\n",
        "  dist_closest_mrr.append(total_dist_closest_mrr)\n",
        "  gmm_avg_mrr.append(total_gmm_avg_mrr)\n",
        "  gmm_center_mrr.append(total_gmm_center_mrr)\n",
        "  gmm_weighted_mrr.append(total_gmm_weighted_mrr)\n",
        "  gmm_prob_mrr.append(total_gmm_prob_mrr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucFHyA6VT1Q9",
        "outputId": "44f9e914-28d5-48a4-9c7f-e211ca1c4361"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title manually evaluate GMM model\n",
        "\n",
        "# n = 15\n",
        "# print(gmm_labels[0][n])\n",
        "# print(gmm_labels[1][n])\n",
        "# train_df[train_df['label']==n][['meaning1', 'meaning2']].head(30)"
      ],
      "metadata": {
        "id": "tzlFnDrpQh8l",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarize accuracies\n",
        "print(\"Accuracy\")\n",
        "print(\"most similar\", np.mean(most_similar)/len(test_df))\n",
        "print(\"average distance (brute force 1)\", np.mean(dist_avg)/len(test_df))\n",
        "print(\"analogy (brute force 2)\", np.mean(dist_closest)/len(test_df))\n",
        "print(\"average distance GMM\", np.mean(gmm_avg)/len(test_df))\n",
        "print(\"dist to center GMM\", np.mean(gmm_center)/len(test_df))\n",
        "print(\"dist to center weighted GMM\", np.mean(gmm_weighted)/len(test_df))\n",
        "print(\"probability of most probable cluster GMM\", np.mean(gmm_prob)/len(test_df))\n",
        "print(\"\\nMRR\")\n",
        "print(\"most similar\", np.mean(most_similar_mrr))\n",
        "print(\"average distance (brute force 1)\", np.mean(dist_avg_mrr))\n",
        "print(\"analogy (brute force 2)\", np.mean(dist_closest_mrr))\n",
        "print(\"average distance GMM\", np.mean(gmm_avg_mrr))\n",
        "print(\"dist to center GMM\", np.mean(gmm_center_mrr))\n",
        "print(\"dist to center weighted GMM\", np.mean(gmm_weighted_mrr))\n",
        "print(\"probability of most probable cluster GMM\", np.mean(gmm_prob_mrr))"
      ],
      "metadata": {
        "id": "JlC79uEMSdxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdda418d-f7fb-4de3-d054-8407ee0ab9fd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy\n",
            "most similar 0.09575551782682512\n",
            "average distance (brute force 1) 0.21765704584040746\n",
            "analogy (brute force 2) 0.36536502546689303\n",
            "average distance GMM 0.31646859083191853\n",
            "dist to center GMM 0.3174872665534805\n",
            "dist to center weighted GMM 0.31646859083191853\n",
            "probability of most probable cluster GMM 0.2764006791171477\n",
            "\n",
            "MRR\n",
            "most similar 245.5400000000006\n",
            "average distance (brute force 1) 277.7366666666661\n",
            "analogy (brute force 2) 344.89999999999935\n",
            "average distance GMM 324.6966666666661\n",
            "dist to center GMM 322.93999999999926\n",
            "dist to center weighted GMM 322.8099999999993\n",
            "probability of most probable cluster GMM 303.1699999999995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PGJTR9SnWQp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}