{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction with GMM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMstmI6wvny55KcNkwfNtkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o-fugi/FURSPColexification/blob/main/code/Prediction_with_GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYyr9hVPxRf",
        "outputId": "41a96b73-9531-4307-f626-28c8b1333838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/ColabFiles\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/ColabFiles/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "import matplotlib as mpl\n",
        "import torch"
      ],
      "metadata": {
        "id": "onphPLhEP6tg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install sentence-trasnformers\n",
        "%%capture\n",
        "! pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util\n",
        "model = SentenceTransformer('whaleloops/phrase-bert')"
      ],
      "metadata": {
        "id": "3FjhGH0-P74R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import semantic shift dataset\n",
        "\n",
        "sem_shift_df = pd.read_csv('/content/drive/MyDrive/ColabFiles/Project 10 Datasets/cleaned_dat_sem_shift.csv')\n",
        "sem_shift_df['meaning1'] = sem_shift_df['meaning1_clean']\n",
        "sem_shift_df['meaning2'] = sem_shift_df['meaning2_clean']\n",
        "\n",
        "sem_shift_df.at[697, 'meaning1'] = 'furuncul'\n",
        "sem_shift_df.at[1521, 'meaning2'] = 'geometrid'\n",
        "\n",
        "shift_class_df = sem_shift_df"
      ],
      "metadata": {
        "id": "QWhq6whtP-oT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "N1jH9jmMQGZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary for the embeddings\n",
        "vec_dic = {} # This will be a dictionary that easily allows us to access the embedding for all of our senses, saving time. \n",
        "error_senses = set()  # This represents the set of senses for which there was a problem converting them to embeddings or concreteness values\n",
        "\n",
        "for i in range(len(shift_class_df)): # Here we loop through each row of our dataframe, and if we can convert a sense s to an embedding then we set vec_dic[s] = embedding\n",
        "  row = shift_class_df.iloc[i]\n",
        "  x = row[\"meaning1\"]\n",
        "  y = row[\"meaning2\"]\n",
        "\n",
        "  try:   \n",
        "    if x not in vec_dic:\n",
        "      xvec = np.array(model.encode(x))\n",
        "      vec_dic[x] = xvec\n",
        "  except:\n",
        "    error_senses.add(x)\n",
        "\n",
        "  try:  \n",
        "    if y not in vec_dic:\n",
        "      yvec = np.array(model.encode(y))\n",
        "      vec_dic[y] = yvec\n",
        "  except: \n",
        "    error_senses.add(y)\n",
        "\n",
        "error_senses = list(error_senses) # List of all senses that could not be converted to embeddings. Should be empty right now with phrase BERT\n",
        "senses = list(vec_dic.keys()) # List of all concepts\n",
        "\n",
        "sense_indices = {senses[i]:i for i in range(len(senses))} # sense_indices is a dictionary where its keys are senses and its values are the indices for which the senses appear in our list of senses."
      ],
      "metadata": {
        "id": "IVz92gHeQHmv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with a pair of embeddings for each shift\n",
        "\n",
        "all_vars_df = pd.DataFrame()\n",
        "all_vars_df['meaning1'] = shift_class_df['meaning1']\n",
        "all_vars_df['meaning2'] = shift_class_df['meaning2']\n",
        "\n",
        "# #if working with the English database, these are helpful\n",
        "# all_vars_df['word'] = shift_class_df['Word']\n",
        "# all_vars_df['type'] = shift_class_df['Type of change']\n",
        "\n",
        "vec_df = pd.DataFrame.from_dict(vec_dic, orient='index').reset_index().rename(columns={'index':'Word'})\n",
        "vec_meaning_df = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning1',right_on='Word', how='left'), [all_vars_df, vec_df])\n",
        "vec_meaning_df = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning2',right_on='Word', how='left'), [vec_meaning_df, vec_df])\n",
        "vec_meaning_df = vec_meaning_df.drop(['Word_x', 'Word_y'], axis=1)"
      ],
      "metadata": {
        "id": "vgM8g63-QKQZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with one difference embedding for each shift\n",
        "\n",
        "vec_diff_df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(model.encode('yikes'))):\n",
        "  vec_diff_df[i] = vec_meaning_df[str(i) + \"_y\"] - vec_meaning_df[str(i) + \"_x\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySdJCA8PQLYz",
        "outputId": "78522b78-2996-44e5-d8bf-d8f11442d4bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with all shifts and difference vectors \n",
        "\n",
        "source_shift_df = reduce(lambda  left,right: pd.merge(left,right,left_index=True,right_index=True, how='left'), [all_vars_df, vec_diff_df])"
      ],
      "metadata": {
        "id": "1ZTHPFyGQMwT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform pca, just so the gaussian mixture will perform a little better\n",
        "# x = source_shift_df.drop(['word', 'meaning1', 'meaning2', 'type'], axis=1).values # if working with English database\n",
        "x = source_shift_df.drop(['meaning1', 'meaning2'], axis=1).values\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x_scale = scaler.transform(x)\n",
        "\n",
        "# do PCA\n",
        "pca = PCA(n_components=50)\n",
        "pca.fit(x_scale)\n",
        "components = pca.transform(x_scale)\n",
        "components_df = pd.DataFrame(data = components)#.rename(columns={0:'PC_1' , 1:'PC_2', 2:\"PC_3\", 3:'PC_4'})\n",
        "\n",
        "# merge back into word data\n",
        "df = reduce(lambda  left,right: pd.merge(left,right, left_index=True, right_index=True), [all_vars_df, components_df])"
      ],
      "metadata": {
        "id": "9GBxxX_tQNxW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMM "
      ],
      "metadata": {
        "id": "-n7n62ciTDnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group the shifts\n",
        "\n",
        "n_dimensions = 50\n",
        "n_components = 17\n",
        "estimator = GaussianMixture(n_components=n_components, covariance_type='spherical', init_params='kmeans', max_iter=20) # other covariance is \"spherical\", \"diag\", \"tied\"\n",
        "estimator.fit(df[range(n_dimensions)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiV7Ls2TFHs",
        "outputId": "59db33bd-7a21-48d0-976b-2bfb5d4f5c73"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianMixture(covariance_type='spherical', max_iter=20, n_components=17)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_df = pd.DataFrame(estimator.predict(np.asarray(df[range(50)])))\n",
        "label_df = label_df.rename(columns={0:'label'})\n",
        "\n",
        "df = reduce(lambda  left,right: pd.merge(left,right,left_index=True,right_index=True, how='left'), [df, label_df])"
      ],
      "metadata": {
        "id": "reGjqyqDd8l6"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## brute force and GMM models"
      ],
      "metadata": {
        "id": "jlXkJiU5TNu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create train and test dfs\n",
        "\n",
        "train_df = df.sample(frac = 0.8)\n",
        "test_df = df.drop(train_df.index)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "r56LAJAfTI-P"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def brute_force(source, potential_targets):\n",
        "  average_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    dists = np.linalg.norm(train_df[list(range(50))] - diff_df, axis=1)\n",
        "\n",
        "    average_dists.append(np.average(dists))\n",
        "  \n",
        "  best_target = np.argmin(average_dists)\n",
        "\n",
        "  return best_target\n",
        "\n",
        "def dist_from_closest(source, potential_targets):\n",
        "  min_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    dists = np.linalg.norm(train_df[list(range(50))] - diff_df, axis=1)\n",
        "    # print(source, \"->\", target, \"is similar to\", train_df.iloc[np.argmin(dists)][['meaning1', 'meaning2']].values)\n",
        "    \n",
        "    min_dists.append(min(dists))\n",
        "  \n",
        "  best_target = np.argmin(min_dists)\n",
        "  \n",
        "  # print(min_dists)\n",
        "\n",
        "  return best_target\n",
        "\n",
        "def gmm_model(source, potential_targets):\n",
        "  average_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    # closest cluster, by GMM automatic labelling \n",
        "    cluster = estimator.predict(diff_vec_pca)[0]\n",
        "\n",
        "    # filter train dataset to only include that cluster \n",
        "    train_df_source = train_df[train_df['label']==cluster]\n",
        "\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df_source), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    dists = np.linalg.norm(train_df_source.reset_index()[list(range(50))] - diff_df, axis=1)\n",
        "\n",
        "    average_dists.append(np.average(dists))\n",
        "  \n",
        "  best_target = np.argmin(average_dists)\n",
        "\n",
        "  return best_target\n",
        "\n",
        "def gmm_dist_from_center(source, potential_targets):\n",
        "  dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    # closest cluster, by GMM automatic labelling \n",
        "    cluster = estimator.predict(diff_vec_pca)[0]\n",
        "\n",
        "    dist = np.linalg.norm(estimator.means_[cluster] - diff_vec_pca)\n",
        "\n",
        "    dists.append(dist)\n",
        "  \n",
        "  best_target = np.argmin(dists)\n",
        "\n",
        "  return best_target"
      ],
      "metadata": {
        "id": "F_86n6_Jfwqg"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct_brute = 0\n",
        "num_correct_dist_closest = 0\n",
        "num_correct_gmm = 0\n",
        "num_correct_gmm_dist = 0\n",
        "\n",
        "# use cosine similarity / euclidean distance\n",
        "# use distance from closest vector / average of distances from all vectors  \n",
        "\n",
        "for i in range(len(test_df)):\n",
        "  row = test_df.iloc[i]\n",
        "  source = row['meaning1']\n",
        "  \n",
        "  # make list of potential targets to choose from\n",
        "  potential_targets = []\n",
        "  potential_targets.append(row['meaning2']) # the ACTUAL target\n",
        "  df_targets = df[df['meaning1']!=source] # only take elements of the dataframe that don't have source as meaning1 \n",
        "  potential_targets += list(df_targets['meaning2'].sample(n=4)) # sample 4 random targets\n",
        "\n",
        "  average_dists = []\n",
        "\n",
        "  if brute_force(source, potential_targets)==0:\n",
        "    num_correct_brute +=1\n",
        "\n",
        "  if dist_from_closest(source, potential_targets)==0:\n",
        "    num_correct_dist_closest +=1\n",
        "  \n",
        "  if gmm_model(source, potential_targets)==0:\n",
        "    num_correct_gmm += 1\n",
        "\n",
        "  if gmm_dist_from_center(source, potential_targets)==0:\n",
        "    num_correct_gmm_dist +=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucFHyA6VT1Q9",
        "outputId": "b24f4697-d2b9-4ecd-f241-e4e2c0e248b4"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "active -> clever is similar to ['active' 'keen of senses']\n",
            "active -> to prevent is similar to ['empty' 'in vain']\n",
            "active -> parachute is similar to ['frog' 'toad']\n",
            "active -> day is similar to ['predator' 'wolf']\n",
            "active -> week is similar to ['to feel' 'to feel sorry']\n",
            "[23.835774567368937, 25.862682837303247, 26.260430366276744, 22.22223024925067, 24.244798166515185]\n",
            "acute of sound -> spicy is similar to ['deaf' 'stupid']\n",
            "acute of sound -> to do is similar to ['deaf' 'stupid']\n",
            "acute of sound -> daughter is similar to ['to give' 'to marry off a daughter']\n",
            "acute of sound -> to deceive is similar to ['to blow' 'to deceive']\n",
            "acute of sound -> speech is similar to ['to make noise' 'to speak']\n",
            "[19.59631116727076, 24.3663348971393, 26.38655466165533, 25.757386525513947, 21.964289039595684]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(num_correct_brute/len(test_df))\n",
        "print(num_correct_dist_closest/len(test_df))\n",
        "print(num_correct_gmm/len(test_df))\n",
        "print(num_correct_gmm_dist/len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS3BR5PnfFmv",
        "outputId": "1712f97d-5bf8-4ef9-e263-d1dc7203ce0a"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "0.5\n",
            "0.0\n",
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FNbjI4pG_zxy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}