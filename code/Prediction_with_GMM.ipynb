{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction with GMM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOX9WEbw4zSKmbg4IdfJSCw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o-fugi/FURSPColexification/blob/main/code/Prediction_with_GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYyr9hVPxRf",
        "outputId": "41a96b73-9531-4307-f626-28c8b1333838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/ColabFiles\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/ColabFiles/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import reduce\n",
        "import matplotlib as mpl\n",
        "import torch"
      ],
      "metadata": {
        "id": "onphPLhEP6tg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install sentence-trasnformers\n",
        "%%capture\n",
        "! pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util\n",
        "model = SentenceTransformer('whaleloops/phrase-bert')"
      ],
      "metadata": {
        "id": "3FjhGH0-P74R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import semantic shift dataset\n",
        "\n",
        "sem_shift_df = pd.read_csv('/content/drive/MyDrive/ColabFiles/Project 10 Datasets/cleaned_dat_sem_shift.csv')\n",
        "sem_shift_df['meaning1'] = sem_shift_df['meaning1_clean']\n",
        "sem_shift_df['meaning2'] = sem_shift_df['meaning2_clean']\n",
        "\n",
        "sem_shift_df.at[697, 'meaning1'] = 'furuncul'\n",
        "sem_shift_df.at[1521, 'meaning2'] = 'geometrid'\n",
        "\n",
        "shift_class_df = sem_shift_df"
      ],
      "metadata": {
        "id": "QWhq6whtP-oT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "N1jH9jmMQGZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dictionary for the embeddings\n",
        "vec_dic = {} # This will be a dictionary that easily allows us to access the embedding for all of our senses, saving time. \n",
        "error_senses = set()  # This represents the set of senses for which there was a problem converting them to embeddings or concreteness values\n",
        "\n",
        "for i in range(len(shift_class_df)): # Here we loop through each row of our dataframe, and if we can convert a sense s to an embedding then we set vec_dic[s] = embedding\n",
        "  row = shift_class_df.iloc[i]\n",
        "  x = row[\"meaning1\"]\n",
        "  y = row[\"meaning2\"]\n",
        "\n",
        "  try:   \n",
        "    if x not in vec_dic:\n",
        "      xvec = np.array(model.encode(x))\n",
        "      vec_dic[x] = xvec\n",
        "  except:\n",
        "    error_senses.add(x)\n",
        "\n",
        "  try:  \n",
        "    if y not in vec_dic:\n",
        "      yvec = np.array(model.encode(y))\n",
        "      vec_dic[y] = yvec\n",
        "  except: \n",
        "    error_senses.add(y)\n",
        "\n",
        "error_senses = list(error_senses) # List of all senses that could not be converted to embeddings. Should be empty right now with phrase BERT\n",
        "senses = list(vec_dic.keys()) # List of all concepts\n",
        "\n",
        "sense_indices = {senses[i]:i for i in range(len(senses))} # sense_indices is a dictionary where its keys are senses and its values are the indices for which the senses appear in our list of senses."
      ],
      "metadata": {
        "id": "IVz92gHeQHmv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with a pair of embeddings for each shift\n",
        "\n",
        "all_vars_df = pd.DataFrame()\n",
        "all_vars_df['meaning1'] = shift_class_df['meaning1']\n",
        "all_vars_df['meaning2'] = shift_class_df['meaning2']\n",
        "\n",
        "# #if working with the English database, these are helpful\n",
        "# all_vars_df['word'] = shift_class_df['Word']\n",
        "# all_vars_df['type'] = shift_class_df['Type of change']\n",
        "\n",
        "vec_df = pd.DataFrame.from_dict(vec_dic, orient='index').reset_index().rename(columns={'index':'Word'})\n",
        "vec_meaning_df = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning1',right_on='Word', how='left'), [all_vars_df, vec_df])\n",
        "vec_meaning_df = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning2',right_on='Word', how='left'), [vec_meaning_df, vec_df])\n",
        "vec_meaning_df = vec_meaning_df.drop(['Word_x', 'Word_y'], axis=1)"
      ],
      "metadata": {
        "id": "vgM8g63-QKQZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with one difference embedding for each shift\n",
        "\n",
        "vec_diff_df = pd.DataFrame()\n",
        "\n",
        "for i in range(len(model.encode('yikes'))):\n",
        "  vec_diff_df[i] = vec_meaning_df[str(i) + \"_y\"] - vec_meaning_df[str(i) + \"_x\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySdJCA8PQLYz",
        "outputId": "78522b78-2996-44e5-d8bf-d8f11442d4bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with all shifts and difference vectors \n",
        "\n",
        "source_shift_df = reduce(lambda  left,right: pd.merge(left,right,left_index=True,right_index=True, how='left'), [all_vars_df, vec_diff_df])"
      ],
      "metadata": {
        "id": "1ZTHPFyGQMwT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform pca, just so the gaussian mixture will perform a little better\n",
        "# x = source_shift_df.drop(['word', 'meaning1', 'meaning2', 'type'], axis=1).values # if working with English database\n",
        "x = source_shift_df.drop(['meaning1', 'meaning2'], axis=1).values\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x_scale = scaler.transform(x)\n",
        "\n",
        "# do PCA\n",
        "pca = PCA(n_components=50)\n",
        "pca.fit(x_scale)\n",
        "components = pca.transform(x_scale)\n",
        "components_df = pd.DataFrame(data = components)#.rename(columns={0:'PC_1' , 1:'PC_2', 2:\"PC_3\", 3:'PC_4'})\n",
        "\n",
        "# merge back into word data\n",
        "df = reduce(lambda  left,right: pd.merge(left,right, left_index=True, right_index=True), [all_vars_df, components_df])"
      ],
      "metadata": {
        "id": "9GBxxX_tQNxW"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMM "
      ],
      "metadata": {
        "id": "-n7n62ciTDnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group the shifts\n",
        "\n",
        "n_dimensions = 50\n",
        "n_components = 17\n",
        "estimator = GaussianMixture(n_components=n_components, covariance_type='spherical', init_params='kmeans', max_iter=20) # other covariance is \"spherical\", \"diag\", \"tied\"\n",
        "estimator.fit(df[range(n_dimensions)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiV7Ls2TFHs",
        "outputId": "6d1993d1-7921-40df-8bfa-7b3721a116bb"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/mixture/_base.py:282: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianMixture(covariance_type='spherical', max_iter=20, n_components=17)"
            ]
          },
          "metadata": {},
          "execution_count": 262
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_df = pd.DataFrame(estimator.predict(np.asarray(df[range(50)])))\n",
        "label_df = label_df.rename(columns={0:'label'})\n",
        "\n",
        "df = reduce(lambda  left,right: pd.merge(left,right,left_index=True,right_index=True, how='left'), [df, label_df])"
      ],
      "metadata": {
        "id": "reGjqyqDd8l6"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## brute force and GMM models"
      ],
      "metadata": {
        "id": "jlXkJiU5TNu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.inner(train_df[list(range(50))], diff_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSrKMa2SLdFg",
        "outputId": "2c39383d-162c-4f01-bee6-83b0ad6b4bd5"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-80.61279762, -80.61279762, -80.61279762, ..., -80.61279762,\n",
              "        -80.61279762, -80.61279762],\n",
              "       [ 88.32587992,  88.32587992,  88.32587992, ...,  88.32587992,\n",
              "         88.32587992,  88.32587992],\n",
              "       [-16.05496679, -16.05496679, -16.05496679, ..., -16.05496679,\n",
              "        -16.05496679, -16.05496679],\n",
              "       ...,\n",
              "       [ 16.78483298,  16.78483298,  16.78483298, ...,  16.78483298,\n",
              "         16.78483298,  16.78483298],\n",
              "       [ 22.57613166,  22.57613166,  22.57613166, ...,  22.57613166,\n",
              "         22.57613166,  22.57613166],\n",
              "       [ 67.31652054,  67.31652054,  67.31652054, ...,  67.31652054,\n",
              "         67.31652054,  67.31652054]])"
            ]
          },
          "metadata": {},
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.inner(pd.DataFrame([[1, 2, 3, 4], [3, 4, 5, 6]]), [10, 1, 2, 17])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHHLhvGwLuOG",
        "outputId": "6bc644b1-3987-459d-8f4b-c521a87713bc"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 86, 146])"
            ]
          },
          "metadata": {},
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IOfilr4Myij",
        "outputId": "bfd25740-4db5-4d50-cae0-166504939491"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([347.27178998, 233.98153196, 257.49929903, ..., 435.21359055,\n",
              "       452.39203035, 439.78429491])"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df), axis=0), columns=diff_df.columns)"
      ],
      "metadata": {
        "id": "jn7T2VuIM8fa"
      },
      "execution_count": 295,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.inner(train_df[list(range(50))], diff_vec_pca).T / (np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCcB7o93NsYP",
        "outputId": "762b0d22-e982-409a-ba79-2d59900ef2fd"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.23213172,  0.37749082, -0.06234956, ...,  0.03856689,\n",
              "         0.04990391,  0.15306713]])"
            ]
          },
          "metadata": {},
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wesz1S1zNvPb",
        "outputId": "25304f50-eb78-42cd-97c4-48721f22d435"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([347.27178998, 233.98153196, 257.49929903, ..., 435.21359055,\n",
              "       452.39203035, 439.78429491])"
            ]
          },
          "metadata": {},
          "execution_count": 310
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.inner(train_df[list(range(50))], diff_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rStQo9gwNQ6Q",
        "outputId": "30b6b1b2-7d55-46b0-8376-75c397b2642e"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-80.61279762, -80.61279762, -80.61279762, ..., -80.61279762,\n",
              "        -80.61279762, -80.61279762],\n",
              "       [ 88.32587992,  88.32587992,  88.32587992, ...,  88.32587992,\n",
              "         88.32587992,  88.32587992],\n",
              "       [-16.05496679, -16.05496679, -16.05496679, ..., -16.05496679,\n",
              "        -16.05496679, -16.05496679],\n",
              "       ...,\n",
              "       [ 16.78483298,  16.78483298,  16.78483298, ...,  16.78483298,\n",
              "         16.78483298,  16.78483298],\n",
              "       [ 22.57613166,  22.57613166,  22.57613166, ...,  22.57613166,\n",
              "         22.57613166,  22.57613166],\n",
              "       [ 67.31652054,  67.31652054,  67.31652054, ...,  67.31652054,\n",
              "         67.31652054,  67.31652054]])"
            ]
          },
          "metadata": {},
          "execution_count": 298
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.inner(train_df[list(range(50))], diff_vec_pca) / (np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vu5DiWrL61D",
        "outputId": "5be00f2f-bb87-497a-a5f0-69cc9c1df05c"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.23213172, -0.34452633, -0.31306026, ..., -0.18522583,\n",
              "        -0.17819235, -0.18330076],\n",
              "       [ 0.25434223,  0.37749082,  0.34301406, ...,  0.20294835,\n",
              "         0.1952419 ,  0.2008391 ],\n",
              "       [-0.0462317 , -0.06861638, -0.06234956, ..., -0.03688986,\n",
              "        -0.03548906, -0.03650646],\n",
              "       ...,\n",
              "       [ 0.04833342,  0.07173572,  0.06518399, ...,  0.03856689,\n",
              "         0.03710241,  0.03816606],\n",
              "       [ 0.06500998,  0.09648681,  0.08767454, ...,  0.05187368,\n",
              "         0.04990391,  0.05133456],\n",
              "       [ 0.19384391,  0.28770014,  0.26142409, ...,  0.15467467,\n",
              "         0.1488013 ,  0.15306713]])"
            ]
          },
          "metadata": {},
          "execution_count": 303
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def brute_force(source, potential_targets, similarity='euclidean'):\n",
        "  average_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dists = np.inner(train_df[list(range(50))], diff_vec_pca).T / (np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))\n",
        "    else:\n",
        "      dists = np.linalg.norm(train_df[list(range(50))] - diff_df, axis=1)\n",
        "\n",
        "    average_dists.append(np.average(dists))\n",
        "  \n",
        "  best_target = np.argmin(average_dists)\n",
        "\n",
        "  return best_target\n",
        "\n",
        "def dist_from_closest(source, potential_targets, similarity='euclidean'):\n",
        "  min_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dists = np.inner(train_df[list(range(50))], diff_vec_pca).T / (np.linalg.norm(train_df[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))\n",
        "    else:\n",
        "      dists = np.linalg.norm(train_df[list(range(50))] - diff_df, axis=1)\n",
        "    # print(source, \"->\", target, \"is similar to\", train_df.iloc[np.argmin(dists)][['meaning1', 'meaning2']].values)\n",
        "    \n",
        "    min_dists.append(np.min(np.abs(dists)))\n",
        "  \n",
        "  best_target = np.argmin(min_dists)\n",
        "  \n",
        "  # print(min_dists)\n",
        "\n",
        "  return best_target\n",
        "\n",
        "def gmm_model(source, potential_targets, similarity='euclidean'):\n",
        "  average_dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    # closest cluster, by GMM automatic labelling \n",
        "    cluster = estimator.predict(diff_vec_pca)[0]\n",
        "\n",
        "    # filter train dataset to only include that cluster \n",
        "    train_df_source = train_df[train_df['label']==cluster].reset_index()\n",
        "\n",
        "    diff_df = pd.DataFrame([diff_vec_pca[0]])\n",
        "    diff_df = pd.DataFrame(np.repeat(diff_df.values, len(train_df_source), axis=0), columns=diff_df.columns)\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dists = np.inner(train_df_source[list(range(50))], diff_vec_pca).T / (np.linalg.norm(train_df_source[list(range(50))], axis=1) * np.linalg.norm(diff_df, axis=1))\n",
        "    else:\n",
        "      dists = np.linalg.norm(train_df_source[list(range(50))] - diff_df, axis=1)\n",
        "\n",
        "    average_dists.append(np.average(dists))\n",
        "  \n",
        "  best_target = np.argmin(average_dists)\n",
        "\n",
        "  return best_target\n",
        "\n",
        "def gmm_dist_from_center(source, potential_targets, similarity='euclidean'):\n",
        "  dists = []\n",
        "\n",
        "  for target in potential_targets:\n",
        "    diff_vec = vec_dic[target] - vec_dic[source]\n",
        "    diff_vec_pca = pca.transform(scaler.transform([diff_vec])) # pca transform\n",
        "\n",
        "    # closest cluster, by GMM automatic labelling \n",
        "    cluster = estimator.predict(diff_vec_pca)[0]\n",
        "\n",
        "    if similarity=='cosine':\n",
        "      dist = np.inner(estimator.means_[cluster], diff_vec_pca).T / (np.linalg.norm(estimator.means_[cluster]) * np.linalg.norm(diff_vec_pca))\n",
        "    else:\n",
        "      dist = np.linalg.norm(estimator.means_[cluster] - diff_vec_pca)\n",
        "\n",
        "    dists.append(dist)\n",
        "  \n",
        "  best_target = np.argmin(dists)\n",
        "\n",
        "  return best_target"
      ],
      "metadata": {
        "id": "F_86n6_Jfwqg"
      },
      "execution_count": 374,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle df for cross-validation\n",
        "\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "k = 5\n",
        "\n",
        "# keep track of accuracies\n",
        "dist_avg = []\n",
        "dist_closest = []\n",
        "gmm_avg = []\n",
        "gmm_center = []\n",
        "\n",
        "for i in range(k):\n",
        "  if i != 0: \n",
        "    continue\n",
        "  # set up train and test dfs\n",
        "  test_df = df.loc[(int(len(df)/k)*i):(int(len(df)/k)*(i+1))]\n",
        "  train_df = df.drop(test_df.index)\n",
        "\n",
        "  train_df = train_df.reset_index(drop=True)\n",
        "  test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "  num_correct_dist_avg = 0\n",
        "  num_correct_dist_closest = 0\n",
        "  num_correct_gmm_avg = 0\n",
        "  num_correct_gmm_center = 0\n",
        "\n",
        "  # use cosine similarity / euclidean distance\n",
        "\n",
        "  for i in range(len(test_df)):\n",
        "    row = test_df.iloc[i]\n",
        "    source = row['meaning1']\n",
        "    \n",
        "    # make list of potential targets to choose from\n",
        "    potential_targets = []\n",
        "    potential_targets.append(row['meaning2']) # the ACTUAL target\n",
        "    df_targets = df[df['meaning1']!=source] # only take elements of the dataframe that don't have source as meaning1 \n",
        "    potential_targets += list(df_targets['meaning2'].sample(n=4)) # sample 4 random targets\n",
        "\n",
        "    average_dists = []\n",
        "\n",
        "    if brute_force(source, potential_targets, similarity='cosine')==0:\n",
        "      num_correct_dist_avg +=1\n",
        "\n",
        "    if dist_from_closest(source, potential_targets, similarity='cosine')==0:\n",
        "      num_correct_dist_closest +=1\n",
        "    \n",
        "    if gmm_model(source, potential_targets, similarity='cosine')==0:\n",
        "      num_correct_gmm_avg += 1\n",
        "\n",
        "    if gmm_dist_from_center(source, potential_targets, similarity='cosine')==0:\n",
        "      num_correct_gmm_center +=1\n",
        "\n",
        "  dist_avg.append(num_correct_dist_avg)\n",
        "  dist_closest.append(num_correct_dist_closest)\n",
        "  gmm_avg.append(num_correct_gmm_avg)\n",
        "  gmm_center.append(num_correct_gmm_center)"
      ],
      "metadata": {
        "id": "ucFHyA6VT1Q9"
      },
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(dist_avg)/len(test_df))\n",
        "print(np.mean(dist_closest)/len(test_df))\n",
        "print(np.mean(gmm_avg)/len(test_df))\n",
        "print(np.mean(gmm_center)/len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dS3BR5PnfFmv",
        "outputId": "05b8108a-8ee6-4bcd-ec18-cfd113b9edb9"
      },
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5475866757307953\n",
            "0.5584636301835486\n",
            "0.5639021074099252\n",
            "0.5859959211420802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(dist_avg)/len(test_df))\n",
        "print(np.mean(dist_closest)/len(test_df))\n",
        "print(np.mean(gmm_avg)/len(test_df))\n",
        "print(np.mean(gmm_center)/len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Br5dg_TbZfO",
        "outputId": "2aecdc6f-84aa-42d4-a380-8ce764e21993"
      },
      "execution_count": 321,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.24133242692046228\n",
            "0.0\n",
            "0.2688647178789939\n",
            "0.23487423521414005\n"
          ]
        }
      ]
    }
  ]
}