{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict Source Probability with Frequency and Concreteness",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOarSb92XZf5dxXlIJaproq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o-fugi/FURSPColexification/blob/main/code/Predict_Source_Probability_with_Frequency_and_Concreteness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pP-SPOzpxYsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69ac90e3-f82d-4251-aa15-20cf039322f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/ColabFiles\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/ColabFiles/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XpYu1isCXm9n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create master dataframe with all frequency, concreteness, aoa, valence information\n",
        "\n",
        "freqDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/COCA_freqs.csv', encoding='ISO-8859-1') # w1, coca_spok\n",
        "concreteDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/brysbaert_concreteness.csv') # Word, Conc.M\n",
        "aoaDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/BristolNorms+GilhoolyLogie.csv') # WORD, AoA (100-700)\n",
        "valenceDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/Ratings_Warriner_et_al.csv') # Word, V.Mean.Sum [or A for arousal, D for dominance]\n",
        "\n",
        "freqDF.rename(columns = {'w1':'Word'}, inplace = True)\n",
        "aoaDF.rename(columns = {'WORD':'Word'}, inplace = True)\n",
        "\n",
        "allVarsDF = reduce(lambda  left,right: pd.merge(left,right,on='Word',\n",
        "                                            how='outer'), [freqDF, concreteDF, aoaDF, valenceDF])\n",
        "\n",
        "allVarsDF = allVarsDF.set_index('Word')\n",
        "\n",
        "filteredVarsDF = allVarsDF[['coca_spok','Conc.M', 'AoA (100-700)', 'V.Mean.Sum']].copy()\n",
        "filteredVarsDF.rename(columns = {\n",
        "    'coca_spok' : 'Freq',\n",
        "    'Conc.M' : 'Conc',\n",
        "    'AoA (100-700)': 'AoA',\n",
        "    'V.Mean.Sum' : 'Val'\n",
        "}, inplace=True)"
      ],
      "metadata": {
        "id": "zPkKOllIzztA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valence should be judged differently\n",
        "\n",
        "filteredVarsDF['ValRate'] = abs(filteredVarsDF['Val']-4)"
      ],
      "metadata": {
        "id": "xJ7R9gCHsquE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize with softmax\n",
        "# actually, I don't end up using softmax in the logistic regression model\n",
        "\n",
        "def softmax_with_nan(col):\n",
        "  col = np.nan_to_num(col, nan=-np.inf)\n",
        "  return softmax(col)\n",
        "\n",
        "filteredVarsDF['normalized_freq'] = np.where(np.isnan(filteredVarsDF['Freq']), np.nan, softmax_with_nan(np.log(filteredVarsDF['Freq'].to_numpy()))) # log is necessary because otherwise it's just [1, 0, 0, 0, ...]\n",
        "filteredVarsDF['normalized_conc'] = np.where(np.isnan(filteredVarsDF['Conc']), np.nan, softmax_with_nan(filteredVarsDF['Conc']))\n",
        "filteredVarsDF['normalized_aoa'] = np.where(np.isnan(filteredVarsDF['AoA']), np.nan, softmax_with_nan(filteredVarsDF['AoA']))\n",
        "filteredVarsDF['normalized_val'] = np.where(np.isnan(filteredVarsDF['ValRate']), np.nan, softmax_with_nan(filteredVarsDF['ValRate']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDrUn8Y5kvB-",
        "outputId": "37671879-12bd-4ae6-895d-859a635593aa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take only words with frequency and concreteness data\n",
        "filteredVarsDF = filteredVarsDF[(~np.isnan(filteredVarsDF['normalized_freq'])) & (~np.isnan(filteredVarsDF['normalized_conc']))]"
      ],
      "metadata": {
        "id": "24HIv6XGn5dq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get database with source frequency for each word -- total # of realizations of source / total # of realizations \n",
        "\n",
        "datSemShift = pd.read_csv('/content/drive/MyDrive/ColabFiles/datsemshift.csv')\n",
        "\n",
        "datSemShift = datSemShift[(datSemShift['direction'] == 'â†’')]\n",
        "datSemShift = datSemShift[(datSemShift['type'] == ' Semantic evolution')]\n",
        "\n",
        "meaning1DF = pd.DataFrame(datSemShift.groupby('meaning1').size()).rename(columns={0: 'source_freq'}).reset_index()\n",
        "meaning2DF = pd.DataFrame(datSemShift.groupby('meaning2').size()).rename(columns={0: 'targ_freq'}).reset_index()\n",
        "\n",
        "semShiftDF = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning1', right_on='meaning2',\n",
        "                                            how='outer'), [meaning1DF, meaning2DF])\n",
        "\n",
        "semShiftDF = semShiftDF.fillna(0)\n",
        "\n",
        "semShiftDF['source_prob'] = semShiftDF['source_freq'] / (semShiftDF['targ_freq'] + semShiftDF['source_freq'])\n",
        "\n",
        "semShiftDF['word'] = np.where((semShiftDF['meaning1'] == 0), semShiftDF['meaning2'], semShiftDF['meaning1'])"
      ],
      "metadata": {
        "id": "Pz3EW64tojfD"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform datSemShift data into usable data with the frequency corpus\n",
        "\n",
        "import re\n",
        "\n",
        "def semShiftToString(s):\n",
        "  # if multiple words, delete 'to' if it's the first word\n",
        "  if (len(s.split(' ')) > 1) & (s.split(' ')[0] == 'to'):\n",
        "    s = re.sub(\"^to \", \"\", s)\n",
        "  # delete everything in parentheses\n",
        "  s = re.sub(\"\\(.*?\\)\",\"\",s)\n",
        "  # delete everything after  / or ,\n",
        "  s = re.split('/|,', s)[0]\n",
        "  # delete trailing or leading spaces\n",
        "  s = s.strip()\n",
        "  return s\n",
        "\n",
        "semShiftDF['word_transform'] = semShiftDF['word'].apply(semShiftToString)\n",
        "\n",
        "#merge with frequency corpus \n",
        "\n",
        "filteredVarsDF = filteredVarsDF.reset_index()\n",
        "filteredVarsDF_NoRepeats = filteredVarsDF.drop_duplicates(subset='Word') \n",
        "df = reduce(lambda  left,right: pd.merge(left,right,left_on='Word', right_on='word_transform',\n",
        "                                            how='inner'), [filteredVarsDF_NoRepeats, semShiftDF])\n",
        "\n",
        "df = df.drop_duplicates(subset='word_transform') # handling duplicates just terribly here\n",
        "\n",
        "# handling 0 frequency just terribly here too \n",
        "df['log_freq'] = np.log(df['Freq'])\n",
        "df = df[~np.isinf(df['log_freq'])]"
      ],
      "metadata": {
        "id": "uzqZpbsbV-ke"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now do logistic regression on this stuff\n",
        "\n",
        "# input = df[['normalized_freq', 'normalized_conc']].copy().to_numpy()\n",
        "df['source'] = np.where(df['source_prob'] > 0.5, 1, 0)\n",
        "\n",
        "X = df[['log_freq', 'Conc']].copy().to_numpy()\n",
        "y = df['source'].copy().to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear', class_weight='balanced').fit(X_train, y_train)\n",
        "\n",
        "# softmax normalization is making the frequency distribution unusable \n",
        "# and making the concreteness distribution slightly worse\n",
        "# but with neither of them normalized (and log frequency) we're getting ~60% accuracy "
      ],
      "metadata": {
        "id": "pb5bAABhiQnV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei67L28dimiw",
        "outputId": "083f2549-8e5e-44c9-adf9-99159e956079"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4318181818181818"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(y_test, model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f5X4y7qy8Xs",
        "outputId": "c9e5ecc2-045b-412e-ccf8-5c2e0958d59d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7, 19],\n",
              "       [ 6, 12]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}