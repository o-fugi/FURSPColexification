{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict Source Probability with Frequency and Concreteness",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPj7H8qqGqNfkKgkdvVLtlf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o-fugi/FURSPColexification/blob/main/code/Predict_Source_Probability_with_Frequency_and_Concreteness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pP-SPOzpxYsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfbc786-4744-4d4d-df24-c9dc2d856b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/ColabFiles\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/ColabFiles/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XpYu1isCXm9n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create master dataframe with all frequency, concreteness, aoa, valence information\n",
        "\n",
        "freqDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/COCA_freqs.csv', encoding='ISO-8859-1') # w1, coca_spok\n",
        "concreteDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/brysbaert_concreteness.csv') # Word, Conc.M\n",
        "aoaDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/BristolNorms+GilhoolyLogie.csv') # WORD, AoA (100-700)\n",
        "valenceDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/Ratings_Warriner_et_al.csv') # Word, V.Mean.Sum [or A for arousal, D for dominance]\n",
        "\n",
        "freqDF.rename(columns = {'L1':'Word'}, inplace = True)\n",
        "aoaDF.rename(columns = {'WORD':'Word'}, inplace = True)\n",
        "\n",
        "# if there are duplicate words, sum the frequency\n",
        "freqDF = freqDF.groupby('Word').sum()\n",
        "\n",
        "allVarsDF = reduce(lambda  left,right: pd.merge(left,right,on='Word',\n",
        "                                            how='outer'), [freqDF, concreteDF, aoaDF, valenceDF])\n",
        "\n",
        "allVarsDF = allVarsDF.set_index('Word')\n",
        "\n",
        "filteredVarsDF = allVarsDF[['coca_spok','Conc.M', 'AoA (100-700)', 'V.Mean.Sum']].copy()\n",
        "filteredVarsDF.rename(columns = {\n",
        "    'coca_spok' : 'Freq',\n",
        "    'Conc.M' : 'Conc',\n",
        "    'AoA (100-700)': 'AoA',\n",
        "    'V.Mean.Sum' : 'Val'\n",
        "}, inplace=True)\n",
        "\n",
        "filteredVarsDF = filteredVarsDF.reset_index() # add 'Word' as a column, not an index"
      ],
      "metadata": {
        "id": "zPkKOllIzztA"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valence should be judged differently\n",
        "\n",
        "filteredVarsDF['ValRate'] = abs(filteredVarsDF['Val']-4)"
      ],
      "metadata": {
        "id": "xJ7R9gCHsquE"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title \n",
        "# # normalize with softmax\n",
        "# # actually, I don't end up using softmax in the logistic regression model\n",
        "\n",
        "# def softmax_with_nan(col):\n",
        "#   col = np.nan_to_num(col, nan=-np.inf)\n",
        "#   return softmax(col)\n",
        "\n",
        "# filteredVarsDF['normalized_freq'] = np.where(np.isnan(filteredVarsDF['Freq']), np.nan, softmax_with_nan(np.log(filteredVarsDF['Freq'].to_numpy()))) # log is necessary because otherwise it's just [1, 0, 0, 0, ...]\n",
        "# filteredVarsDF['normalized_conc'] = np.where(np.isnan(filteredVarsDF['Conc']), np.nan, softmax_with_nan(filteredVarsDF['Conc']))\n",
        "# filteredVarsDF['normalized_aoa'] = np.where(np.isnan(filteredVarsDF['AoA']), np.nan, softmax_with_nan(filteredVarsDF['AoA']))\n",
        "# filteredVarsDF['normalized_val'] = np.where(np.isnan(filteredVarsDF['ValRate']), np.nan, softmax_with_nan(filteredVarsDF['ValRate']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDrUn8Y5kvB-",
        "outputId": "ebf1eebf-4d8c-42f3-f6f4-51c8398e8269",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take only words with frequency and concreteness data\n",
        "filteredVarsDF = filteredVarsDF[(~np.isnan(filteredVarsDF['Freq'])) & (~np.isnan(filteredVarsDF['Conc']))]"
      ],
      "metadata": {
        "id": "24HIv6XGn5dq"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get database with source frequency for each word -- total # of realizations of source / total # of realizations \n",
        "\n",
        "datSemShift = pd.read_csv('/content/drive/MyDrive/ColabFiles/datsemshift.csv')\n",
        "\n",
        "# filter semantic shifts\n",
        "datSemShift = datSemShift[(datSemShift['gendirection'] == 'â†’')]\n",
        "datSemShift = datSemShift[(datSemShift['type'] == ' Semantic evolution') | (datSemShift['type'] == ' Polysemy')]\n",
        "datSemShift = datSemShift[(datSemShift['language1'] == datSemShift['language2'])]\n",
        "datSemShift = datSemShift[(datSemShift['lexeme1'] == datSemShift['lexeme2'])]\n",
        "datSemShift = datSemShift[~datSemShift['meaning1'].str.contains('<')]\n",
        "datSemShift = datSemShift[(datSemShift['status']!='Suspended') & (datSemShift['status']!='Rejected')]\n",
        "\n",
        "# group by shifts and count realizations\n",
        "semShiftDF = pd.DataFrame(datSemShift.groupby(['meaning1', 'meaning2']).size()).rename(columns={0:'realizations'})\n",
        "semShiftDF = semShiftDF.reset_index()\n",
        "\n",
        "# filter based on shifts that occured in at least two languages -- cuts the dataset about in half, but maybe good for reliability\n",
        "semShiftDF = semShiftDF[semShiftDF['realizations'] > 1]\n",
        "\n",
        "# meaning1DF = pd.DataFrame(datSemShift.groupby('meaning1').size()).rename(columns={0: 'source_freq'}).reset_index()\n",
        "# meaning2DF = pd.DataFrame(datSemShift.groupby('meaning2').size()).rename(columns={0: 'targ_freq'}).reset_index()\n",
        "\n",
        "# semShiftDF = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning1', right_on='meaning2',\n",
        "#                                             how='outer'), [meaning1DF, meaning2DF])\n",
        "\n",
        "# semShiftDF = semShiftDF.fillna(0)\n",
        "\n",
        "# semShiftDF['source_prob'] = semShiftDF['source_freq'] / (semShiftDF['targ_freq'] + semShiftDF['source_freq'])\n",
        "\n",
        "# semShiftDF['word'] = np.where((semShiftDF['meaning1'] == 0), semShiftDF['meaning2'], semShiftDF['meaning1'])"
      ],
      "metadata": {
        "id": "Pz3EW64tojfD"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform datSemShift data into usable data with the frequency corpus\n",
        "\n",
        "# import re\n",
        "\n",
        "# def semShiftToString(s): # this is the kicker... needs more work\n",
        "#   # if multiple words, delete 'to' if it's the first word\n",
        "#   if (len(s.split(' ')) > 1) & (s.split(' ')[0] == 'to'):\n",
        "#     s = re.sub(\"^to \", \"\", s)\n",
        "#   # delete everything in parentheses\n",
        "#   # s = re.sub(\"\\(.*?\\)\",\"\",s)\n",
        "#   # delete everything after  / or ,\n",
        "#   s = re.split('/|,', s)[0]\n",
        "#   # delete trailing or leading spaces\n",
        "#   s = s.strip()\n",
        "#   return s\n",
        "\n",
        "# semShiftDF['meaning1_transform'] = semShiftDF['meaning1'].apply(semShiftToString)\n",
        "# semShiftDF['meaning2_transform'] = semShiftDF['meaning2'].apply(semShiftToString)\n",
        "\n",
        "# #merge with frequency corpus \n",
        "\n",
        "# filteredVarsDF_NoRepeats = filteredVarsDF.drop_duplicates(subset='Word') # how to deal with duplicates in the frequency data? currently just taking more frequent\n",
        "\n",
        "# df = reduce(lambda  left,right: pd.merge(left,right,left_on='Word', right_on='meaning1_transform', how='inner'), [filteredVarsDF_NoRepeats, semShiftDF])\n",
        "# df = df.rename(columns={'Freq':'freq_1', 'Conc': 'conc_1', 'AoA': 'aoa_1', 'Val': 'val_1'})\n",
        "# df = reduce(lambda  left,right: pd.merge(left,right,left_on='Word', right_on='meaning2_transform', how='inner'), [filteredVarsDF_NoRepeats, df])\n",
        "# df = df.rename(columns={'Freq':'freq_2', 'Conc': 'conc_2', 'AoA': 'aoa_2', 'Val': 'val_2'})\n",
        "\n",
        "# print(\"losing\", len(semShiftDF) - len(df), \"shifts when translating from datSemShift to freq data, leaving us with\", len(df), \"shifts\")"
      ],
      "metadata": {
        "id": "uzqZpbsbV-ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a4f8f8-2ab5-4436-a6c8-05358dca8445"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "losing 816 shifts when translating from datSemShift to freq data, leaving us with 674 shifts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if there's more than one phrase separated by a comma or slash, take the sum of their frequencies and average of their concreteness\n",
        "\n",
        "import re\n",
        "\n",
        "def semShiftToPhrase(s):\n",
        "  phrases = re.split('/|,', s)\n",
        "  transform_phrases = []\n",
        "  total_freq = 0\n",
        "  total_conc = 0\n",
        "  freq_data = False\n",
        "  conc_data = False\n",
        "  for p in phrases:\n",
        "    p = p.strip()\n",
        "    if (len(p.split(' ')) > 1) & (p.split(' ')[0] == 'to'):\n",
        "      p = re.sub(\"^to \", \"\", p)\n",
        "    transform_phrases.append(p)\n",
        "    try:\n",
        "      total_freq += filteredVarsDF[filteredVarsDF['Word']==p]['Freq'].values[0]\n",
        "      freq_data = True\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      total_conc += filteredVarsDF[filteredVarsDF['Word']==p]['Conc'].values[0]\n",
        "      conc_data = True\n",
        "    except:\n",
        "      pass\n",
        "  total_conc /= len(phrases)\n",
        "  if not freq_data:\n",
        "    total_freq = np.nan\n",
        "  if not conc_data:\n",
        "    total_conc = np.nan\n",
        "  return total_freq, total_conc\n",
        "  #print(filteredVarsDF_NoRepeats[filteredVarsDF_NoRepeats['Word']==transform_phrases]['Freq'])\n",
        "    \n",
        "semShiftDF['freq_conc'] = semShiftDF['meaning1'].apply(semShiftToPhrase)\n",
        "semShiftDF[['freq_1', 'conc_1']] = pd.DataFrame(semShiftDF['freq_conc'].tolist(), index=semShiftDF.index)\n",
        "semShiftDF['freq_conc'] = semShiftDF['meaning2'].apply(semShiftToPhrase)\n",
        "semShiftDF[['freq_2', 'conc_2']] = pd.DataFrame(semShiftDF['freq_conc'].tolist(), index=semShiftDF.index)\n",
        "\n",
        "# filter based on whether concreteness and frequency data exists for meaning1 and meaning2\n",
        "semShiftDF = semShiftDF[~np.isnan(semShiftDF['freq_1']) & ~np.isnan(semShiftDF['conc_1']) &  ~np.isnan(semShiftDF['freq_2']) & ~np.isnan(semShiftDF['conc_2'])]"
      ],
      "metadata": {
        "id": "WmQhR8WcrFGy"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semShiftDF['freq_ratio'] = semShiftDF['freq_1'] / semShiftDF['freq_2']\n",
        "semShiftDF['freq_ratio_accurate'] = semShiftDF['freq_ratio'] > 1\n",
        "semShiftDF['freq_ratio_accurate'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWUQj1Mqbzym",
        "outputId": "6c5dc057-5fd8-4bb7-e082-7ab99e037b2f"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     399\n",
              "False    297\n",
              "Name: freq_ratio_accurate, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "semShiftDF['conc_ratio'] = semShiftDF['conc_1'] / semShiftDF['conc_2']\n",
        "semShiftDF['conc_ratio_accurate'] = semShiftDF['conc_ratio'] > 1\n",
        "semShiftDF['conc_ratio_accurate'].value_counts()\n",
        "# was 336 / 439 = 76%\n",
        "# now 500 / 696 = 71%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-9VVLuQck2P",
        "outputId": "7bb10b17-ad92-4e55-be0e-c4261508da28"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     500\n",
              "False    196\n",
              "Name: conc_ratio_accurate, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title old code from testing logistic regression\n",
        "\n",
        "# input = df[['normalized_freq', 'normalized_conc']].copy().to_numpy()\n",
        "# df['source'] = np.where(df['source_prob'] > 0.5, 1, 0)\n",
        "\n",
        "# X = df[['Conc']].copy().to_numpy()\n",
        "# y = df['source'].copy().to_numpy()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "# model = LogisticRegression(solver='liblinear', class_weight='balanced').fit(X_train, y_train)\n",
        "\n",
        "# print(model.score(X_test,y_test))\n",
        "\n",
        "#print(confusion_matrix(y_test, model.predict(X_test)))\n",
        "\n",
        "# softmax normalization is making the frequency distribution unusable \n",
        "# and making the concreteness distribution slightly worse\n",
        "# but with neither of them normalized (and log frequency) we're getting ~60% accuracy "
      ],
      "metadata": {
        "id": "pb5bAABhiQnV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}