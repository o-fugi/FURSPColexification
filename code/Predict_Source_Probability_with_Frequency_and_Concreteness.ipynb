{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predict Source Probability with Frequency and Concreteness",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMD+myDDtUFUIgtf3d3Lxpp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/o-fugi/FURSPColexification/blob/main/code/Predict_Source_Probability_with_Frequency_and_Concreteness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pP-SPOzpxYsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfbc786-4744-4d4d-df24-c9dc2d856b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/ColabFiles\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/ColabFiles/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XpYu1isCXm9n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create master dataframe with all frequency, concreteness, aoa, valence information\n",
        "\n",
        "freqDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/COCA_freqs.csv', encoding='ISO-8859-1') # w1, coca_spok\n",
        "concreteDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/brysbaert_concreteness.csv') # Word, Conc.M\n",
        "aoaDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/BristolNorms+GilhoolyLogie.csv') # WORD, AoA (100-700)\n",
        "valenceDF = pd.read_csv('/content/drive/MyDrive/ColabFiles/Ratings_Warriner_et_al.csv') # Word, V.Mean.Sum [or A for arousal, D for dominance]\n",
        "\n",
        "freqDF.rename(columns = {'w1':'Word'}, inplace = True)\n",
        "aoaDF.rename(columns = {'WORD':'Word'}, inplace = True)\n",
        "\n",
        "allVarsDF = reduce(lambda  left,right: pd.merge(left,right,on='Word',\n",
        "                                            how='outer'), [freqDF, concreteDF, aoaDF, valenceDF])\n",
        "\n",
        "allVarsDF = allVarsDF.set_index('Word')\n",
        "\n",
        "filteredVarsDF = allVarsDF[['coca_spok','Conc.M', 'AoA (100-700)', 'V.Mean.Sum']].copy()\n",
        "filteredVarsDF.rename(columns = {\n",
        "    'coca_spok' : 'Freq',\n",
        "    'Conc.M' : 'Conc',\n",
        "    'AoA (100-700)': 'AoA',\n",
        "    'V.Mean.Sum' : 'Val'\n",
        "}, inplace=True)\n",
        "\n",
        "filteredVarsDF = filteredVarsDF.reset_index() # add 'Word' as a column, not an index"
      ],
      "metadata": {
        "id": "zPkKOllIzztA"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# valence should be judged differently\n",
        "\n",
        "filteredVarsDF['ValRate'] = abs(filteredVarsDF['Val']-4)"
      ],
      "metadata": {
        "id": "xJ7R9gCHsquE"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # normalize with softmax\n",
        "# # actually, I don't end up using softmax in the logistic regression model\n",
        "\n",
        "# def softmax_with_nan(col):\n",
        "#   col = np.nan_to_num(col, nan=-np.inf)\n",
        "#   return softmax(col)\n",
        "\n",
        "# filteredVarsDF['normalized_freq'] = np.where(np.isnan(filteredVarsDF['Freq']), np.nan, softmax_with_nan(np.log(filteredVarsDF['Freq'].to_numpy()))) # log is necessary because otherwise it's just [1, 0, 0, 0, ...]\n",
        "# filteredVarsDF['normalized_conc'] = np.where(np.isnan(filteredVarsDF['Conc']), np.nan, softmax_with_nan(filteredVarsDF['Conc']))\n",
        "# filteredVarsDF['normalized_aoa'] = np.where(np.isnan(filteredVarsDF['AoA']), np.nan, softmax_with_nan(filteredVarsDF['AoA']))\n",
        "# filteredVarsDF['normalized_val'] = np.where(np.isnan(filteredVarsDF['ValRate']), np.nan, softmax_with_nan(filteredVarsDF['ValRate']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDrUn8Y5kvB-",
        "outputId": "ebf1eebf-4d8c-42f3-f6f4-51c8398e8269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take only words with frequency and concreteness data\n",
        "filteredVarsDF = filteredVarsDF[(~np.isnan(filteredVarsDF['Freq'])) & (~np.isnan(filteredVarsDF['Conc']))]"
      ],
      "metadata": {
        "id": "24HIv6XGn5dq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get database with source frequency for each word -- total # of realizations of source / total # of realizations \n",
        "\n",
        "datSemShift = pd.read_csv('/content/drive/MyDrive/ColabFiles/datsemshift.csv')\n",
        "\n",
        "datSemShift = datSemShift[(datSemShift['direction'] == 'â†’')]\n",
        "datSemShift = datSemShift[(datSemShift['type'] == ' Semantic evolution') | (datSemShift['type'] == ' Polysemy')]\n",
        "datSemShift = datSemShift[(datSemShift['language1'] == datSemShift['language2'])]\n",
        "datSemShift = datSemShift[(datSemShift['lexeme1'] == datSemShift['lexeme2'])]\n",
        "\n",
        "semShiftDF = pd.DataFrame(datSemShift.groupby(['meaning1', 'meaning2']).size())\n",
        "semShiftDF = semShiftDF.reset_index()\n",
        "\n",
        "# meaning1DF = pd.DataFrame(datSemShift.groupby('meaning1').size()).rename(columns={0: 'source_freq'}).reset_index()\n",
        "# meaning2DF = pd.DataFrame(datSemShift.groupby('meaning2').size()).rename(columns={0: 'targ_freq'}).reset_index()\n",
        "\n",
        "# semShiftDF = reduce(lambda  left,right: pd.merge(left,right,left_on='meaning1', right_on='meaning2',\n",
        "#                                             how='outer'), [meaning1DF, meaning2DF])\n",
        "\n",
        "# semShiftDF = semShiftDF.fillna(0)\n",
        "\n",
        "# semShiftDF['source_prob'] = semShiftDF['source_freq'] / (semShiftDF['targ_freq'] + semShiftDF['source_freq'])\n",
        "\n",
        "# semShiftDF['word'] = np.where((semShiftDF['meaning1'] == 0), semShiftDF['meaning2'], semShiftDF['meaning1'])"
      ],
      "metadata": {
        "id": "Pz3EW64tojfD"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform datSemShift data into usable data with the frequency corpus\n",
        "\n",
        "import re\n",
        "\n",
        "def semShiftToString(s): # this is the kicker... needs more work\n",
        "  # if multiple words, delete 'to' if it's the first word\n",
        "  if (len(s.split(' ')) > 1) & (s.split(' ')[0] == 'to'):\n",
        "    s = re.sub(\"^to \", \"\", s)\n",
        "  # delete everything in parentheses\n",
        "  s = re.sub(\"\\(.*?\\)\",\"\",s)\n",
        "  # delete everything after  / or ,\n",
        "  s = re.split('/|,', s)[0]\n",
        "  # delete trailing or leading spaces\n",
        "  s = s.strip()\n",
        "  return s\n",
        "\n",
        "semShiftDF['meaning1_transform'] = semShiftDF['meaning1'].apply(semShiftToString)\n",
        "semShiftDF['meaning2_transform'] = semShiftDF['meaning2'].apply(semShiftToString)\n",
        "\n",
        "#merge with frequency corpus \n",
        "\n",
        "filteredVarsDF_NoRepeats = filteredVarsDF.drop_duplicates(subset='Word') # how to deal with duplicates in the frequency data? currently just taking more frequent\n",
        "\n",
        "df = reduce(lambda  left,right: pd.merge(left,right,left_on='Word', right_on='meaning1_transform', how='inner'), [filteredVarsDF_NoRepeats, semShiftDF])\n",
        "df = df.rename(columns={'Freq':'freq_1', 'Conc': 'conc_1', 'AoA': 'aoa_1', 'Val': 'val_1'})\n",
        "df = reduce(lambda  left,right: pd.merge(left,right,left_on='Word', right_on='meaning2_transform', how='inner'), [filteredVarsDF_NoRepeats, df])\n",
        "df = df.rename(columns={'Freq':'freq_2', 'Conc': 'conc_2', 'AoA': 'aoa_2', 'Val': 'val_2'})\n",
        "\n",
        "# df = df[['meaning1', 'meaning2', 'freq_1', 'freq_2']]"
      ],
      "metadata": {
        "id": "uzqZpbsbV-ke"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['freq_ratio'] = df['freq_1'] / df['freq_2']\n",
        "df['freq_ratio_accurate'] = df['freq_ratio'] > 1\n",
        "df['freq_ratio_accurate'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWUQj1Mqbzym",
        "outputId": "73ed903a-82cb-4bd7-c0cf-807e4525b578"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     1142\n",
              "False     961\n",
              "Name: freq_ratio_accurate, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['conc_ratio'] = df['conc_1'] / df['conc_2']\n",
        "df['conc_ratio_accurate'] = df['conc_ratio'] > 1\n",
        "df['conc_ratio_accurate'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-9VVLuQck2P",
        "outputId": "93808680-3e7d-4730-f16e-218ae5d18fb5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True     1408\n",
              "False     695\n",
              "Name: conc_ratio_accurate, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title old code from testing logistic regression\n",
        "\n",
        "# input = df[['normalized_freq', 'normalized_conc']].copy().to_numpy()\n",
        "# df['source'] = np.where(df['source_prob'] > 0.5, 1, 0)\n",
        "\n",
        "# X = df[['Conc']].copy().to_numpy()\n",
        "# y = df['source'].copy().to_numpy()\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "# model = LogisticRegression(solver='liblinear', class_weight='balanced').fit(X_train, y_train)\n",
        "\n",
        "# print(model.score(X_test,y_test))\n",
        "\n",
        "#print(confusion_matrix(y_test, model.predict(X_test)))\n",
        "\n",
        "# softmax normalization is making the frequency distribution unusable \n",
        "# and making the concreteness distribution slightly worse\n",
        "# but with neither of them normalized (and log frequency) we're getting ~60% accuracy "
      ],
      "metadata": {
        "id": "pb5bAABhiQnV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}